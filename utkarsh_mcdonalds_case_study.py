# -*- coding: utf-8 -*-
"""Utkarsh_mcdonalds_case_study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k2KRqcA2pUcBISQk2k8E5lqaSs-rzlnz

**<h1> Mcdonalds Case Study<h1>**

### Step-3 = Collecting Dataset
"""

import pandas as pd
import numpy as np

mcd_data = pd.read_csv('/content/drive/MyDrive/Feynn/mcdonalds_case_study.csv')

"""### **Step-4= Exploring Dataset**"""

mcd_data.columns.tolist()

mcd_data.isnull().sum()

for column in mcd_data.columns:
    distinct_values = mcd_data[column].unique()
    print(f"Column '{column}' has {len(distinct_values)} distinct values:")
    print(distinct_values)
    print()

mcd_data.shape

mcd_data.head(3)

# Extract columns 1 to 11 and convert to a binary matrix
MD_x = mcd_data.iloc[:, 0:11].eq('Yes').astype(int)

# Calculate the column means and round to two decimal places
result = np.round(MD_x.mean(), 2)

result

"""**Exploratory Data Analysis**"""

from sklearn.decomposition import PCA
pca = PCA()
MD_pca = pca.fit_transform(MD_x)

# Print the summary
print("Standard deviation of each principal component:")
print(np.sqrt(pca.explained_variance_))
print("\nProportion of variance explained by each principal component:")
print(pca.explained_variance_ratio_)
print("\nCumulative proportion of variance explained:")
print(np.cumsum(pca.explained_variance_ratio_))

pca = PCA()
MD_pca = pca.fit_transform(MD_x)

row= ['stddev','prop_var','cum_var']
Compon = [np.sqrt(pca.explained_variance_),pca.explained_variance_ratio_,np.cumsum(pca.explained_variance_ratio_)]


principal_components_df = pd.DataFrame(data=Compon, columns=[f"PC{i+1}" for i in range(MD_pca.shape[1])], index=row)


print(principal_components_df)

factor_loadings = pca.components_

# Create a DataFrame to store the factor loadings
factor_loadings_df = pd.DataFrame(factor_loadings, columns=principal_components_df.columns, index =MD_x.columns)
factor_loadings_df

import matplotlib.pyplot as plt

pca_df = pd.DataFrame(data=MD_pca, columns=[f"PC{i+1}" for i in range(MD_pca.shape[1])])

# Create a scatter plot of the data points projected onto the principal components
plt.scatter(pca_df["PC1"], pca_df["PC2"], c="grey")

# Get the principal axes (components) of the PCA model
principal_axes = pca.components_

# Plot the principal axes as lines
for i, (x, y) in enumerate(zip(principal_axes[0], principal_axes[1]), start=1):
    plt.arrow(0, 0, x*2.5, y*2.5, head_width=0.2, head_length=0.1, fc='black', ec='black')
    plt.text(x*2.5, y*2.5, f"PC{i}", ha="left", va="top", fontsize=9)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Scatter Plot with Principal Axes")
plt.grid(True)
plt.show()

!pip install bioinfokit
from bioinfokit.visuz import cluster
cluster.screeplot(obj=[factor_loadings_df.columns, pca.explained_variance_ratio_],show=True,dim=(10,5))

pca_scores = PCA().fit_transform(MD_x)

# get 2D biplot
cluster.biplot(cscore=pca_scores, loadings=principal_axes, labels=MD_x.columns, var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,5))

"""## Step-5 = Extracting Segments"""

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(2,8))

visualizer.fit(MD_x)        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure

kmeans_model = KMeans(n_clusters=4, init='k-means++', random_state=0)
kmeans_model.fit_predict(MD_x)
mcd_data['cluster_num'] = kmeans_model.labels_ #adding to df
mcd_data

from sklearn import preprocessing

pca_data = preprocessing.scale(MD_x)

pca = PCA(n_components=11)
pc = pca.fit_transform(MD_x)
names = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11']
pf = pd.DataFrame(data = pc, columns = names)
pf['cluster_num'] = kmeans_model.labels_



plt.scatter(pf['pc1'], pf['pc2'], c=kmeans_model.labels_, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('KMeans Clustering Visualization')
plt.legend()
plt.show()

from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage

# Assuming vacmot is a numpy array or pandas DataFrame
mcd_vdist = pdist(pf.T)
mcd_vclust = linkage(mcd_vdist, method='ward')

from scipy.cluster.hierarchy import linkage, dendrogram
distance_matrix = np.array(MD_x)
distance_matrix =distance_matrix .T
Z = linkage(distance_matrix, method='ward')  # 'ward' linkage uses Ward's minimum variance method

Z.shape

MD_x_T=MD_x.T
MD_x_T.index.shape

MD_x_T

"""### **Step-6 : Profiling Segments**"""

plt.figure(figsize=(10, 6))
dendrogram(Z, labels=MD_x_T.index, orientation='top')
plt.xlabel('Features')
plt.ylabel('Distance')
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

MD_x['cluster_num']= kmeans_model.labels_


cluster_1_df = MD_x[MD_x["cluster_num"]==0]
cluster_1_df.head()



sns.set(style="whitegrid")

# Create a 1x2 panel plot for two segments
nrows, ncols = 2, 2
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(25, 15))

# Flatten axes to loop through them easily
axes_flat = axes.flatten()

# List of unique segment values
segments = MD_x['cluster_num'].unique()

# Create a bar plot for each segment
for segment, ax in zip(segments, axes_flat):
    segment_data = MD_x[MD_x['cluster_num'] == segment]
    sns.barplot(data=segment_data.iloc[:,:-1], ax=ax)
    ax.set_title(f'Segment {segment}')

# Adjust layout and show the plot

plt.show()

mcd_data['Like'].value_counts()

"""### **Step-7 : Describing Segments**"""

mcd_data['Like']= mcd_data["Like"].replace({'I hate it!-5':'-5','I love it!+5':'+5'})

mcd_data['VisitFrequency'].value_counts()

mcd_data['Gender'].value_counts()

from statsmodels.graphics.mosaicplot import mosaic
mosaic(mcd_data, ['Like', 'cluster_num'], gap=0.03, title='Desirability of Mcdonalds')

plt.show()

mosaic(mcd_data, [ 'cluster_num','Gender'], gap=0.03, title='Gender proportions')

plt.show()

sns.boxplot(x='cluster_num', y='Age', data=mcd_data)

# Set labels and title
plt.xlabel('Segment')
plt.ylabel('Age')
plt.title('Age Proportion of Segements')

# Show the plot
plt.show()

"""### **Step-8 : Selecting a Segment**"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn import tree

X = MD_x.iloc[:,:-1]
y = MD_x['cluster_num']

# Split the data into training and testing sets (optional)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the decision tree model
clsf = tree.DecisionTreeClassifier(random_state=1234)

# Fit the model on the data
model = clsf.fit(X, y)

# Plot the decision tree
plt.figure(figsize=(10, 8))
plot_tree(clsf, feature_names=X.columns, filled=True, rounded=True)
plt.title('Decision Tree Visualization')
plt.show()

!pip install graphviz

import graphviz
dot_data = tree.export_graphviz(clsf, out_file=None)
graph = graphviz.Source(dot_data)
graph

from sklearn.metrics import classification_report
from sklearn import metrics


# Create the decision tree model
clsf = DecisionTreeClassifier()

# Fit the model on the data
Decision_model = clsf.fit(X, y)
y_pred = Decision_model.predict(X_test)
print(metrics.confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import pickle
filename = 'Mcdonalds_case_study_cluster'
pickle.dump(Decision_model, open(filename, 'wb'))

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result,'% Acuuracy')

